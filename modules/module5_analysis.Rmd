% R Bootcamp, Module 5: Useful Stuff
% August 2016, UC Berkeley
% Rochelle Terman (rterman@gmail.com)

```{r chunksetup, include=FALSE} 
# include any code here you don't want to show up in the document,
# e.g. package and dataset loading
library(plyr)
library(reshape2)
library(dplyr)
library(tidyr)
library(ggplot2)
library(lmtest)
library(sandwich)
#if(!('modules' %in% unlist(strsplit(getwd(), split = '/')))) setwd('modules')
setwd("~/Dropbox/berkeley/Git-Repos/r-bootcamp-2016/modules")
```

# What exactly is "useful stuff" in R?

For some, it might just be basic calculations

```{r}
63.24 * pi # Multiply 63.24 by pi
exp(x = 4.39) # Raise e to the power of 4.39
log(x = 1.7) # Take the log of 1.7
tan(x = 58) # Compute the tangent of 58
```

For others, it might be large or complex mathematical operations

```{r}
# Take one million samples from the standard normal distribution
data.sample <- rnorm(n=1000000, mean=0, sd=1) 

# Build a 1000 x 1000 matrix from the sample data
big.matrix <- matrix(data=data.sample, ncol=1000) 

dim(x = big.matrix) # Confirm that "big.matrix" is 1000 x 1000
big.matrix.inverse <- solve(a=big.matrix) # Compute the inverse of "big.matrix"
system.time(expr = solve(a=big.matrix)) # Compute time required to invert "big.matrix"
```

# Useful Stuff: Applied Research Edition

For most applied researchers, "useful stuff" that can be done in R boils down to a few core items: 

1) ***Tidying*** data into the appropriate format
2) Carrying out operations and calculations across ***groups***
3) Attempting to ***describe relationships*** or conduct ***causal inference*** 

# Group-wise Operations/example dataset

- The "tips" dataset was originally constructed by a waiter who recorded information about the tips he received over a period of several months

- The dataset can be found in the *reshape2* package and originally appeared in:

Bryant, P. G. and Smith, M (1995), *Practical Data Analysis: Case Studies in Business Statistics.* Homewood, IL: Richard D. Irwin Publishing

```{r}
library(reshape2)
data("tips", package = "reshape2")
# Get the object class
class(x = tips)
# Get the object dimensionality 
dim(x = tips) # Note this is rows by columns
# Get the column names
colnames(x = tips)
# View first six rows and all columns
head(x = tips)
# Get detailed column-by-column information
str(object = tips)
```

# Group-wise Operations/Common Calculations

- A good place to start with our data is to calculate summary statistics

- Some notes on computing summary statistics:
1) Note that these functions are sensitive to missing values (NA); you should be sure to specify na.rm=T to avoid errors 

```{r}
# Sample 100 times from the standard normal distribution 
sample.data <- rnorm(n=100, mean=0, sd=1)

# Attempt to calculate the sample mean (absence of NAs)
mean(x = sample.data)

# Add some missing values to the sample
sample.data[c(1,4,16,64)] <- NA

# Attempt to calculate the sample mean (presence of NAs)
mean(x = sample.data) # Action for NAs is not specified
mean(x = sample.data, na.rm = TRUE) # Action for NAs is not specified
```

2) These functions are also sensitive to the presence of factor variables; remove the factor levels to avoid errors (usually use one of as.vector(), as.character(), or as.numeric())

```{r}
# Get a random sample of zeroes and ones 
sample.data <- sample(x = c(0,1), size = 100, replace = T)

# Add factor levels to the sample
sample.data <- factor(x = sample.data)

# Attempt to calculate the sample mean (with factor levels)
mean(x = sample.data)

# Remove factor levels
sample.data <- as.numeric(x = sample.data)

# Check that there are no more factor levels in the sample data
is.factor(x=sample.data)

# Attempt to calculate the sample mean (without factor levels)
mean(x=sample.data, na.rm = T)
```

- Computing some typical summary statistics:
```{r}
# Mean
mean(x=tips$tip, na.rm=T)
# Median
median(x=tips$tip, na.rm=T)
# Standard Deviation
sd(x=tips$tip, na.rm=T)
# Quartiles
quantile(x=tips$tip, na.rm=T, probs=seq(from=0, to=1, by=0.25))
# Quintiles
quantile(x=tips$tip, na.rm=T, probs=seq(from=0, to=1, by=0.2))
# Deciles
quantile(x=tips$tip, na.rm=T, probs=seq(from=0, to=1, by=0.1))
# Percentiles
quantile(x=tips$tip, na.rm=T, probs=seq(from=0, to=1, by=0.01))
```

- We could do the same thing for lots of variables, but there is an easier way!
```{r}
# Compute standard summary statistics for object "red.blue"
summary(object=tips)
```

- Unfortunately, the built-in summary methods don't always pickup every statistic of interest (for example, certain frequencies)
- For this, the table function is very helpful
```{r}
# Isolate the gender column
gender <- tips$sex

# Get a vector of counts for unique values, divide by total count
table(gender)
table(gender)/length(gender)
table(gender)/length(gender)*100

# Isolate the gender and smoker columns
gender.smoker <- tips[,c("sex", "smoker")]

# Get a vector of counts for unique values, divide by total count
table(gender.smoker)
table(gender.smoker)/nrow(gender.smoker)
table(gender.smoker)/nrow(gender.smoker)*100
```

- But it isn't always the case that a) R has a built-in method suting our needs, or b) the number of groups is small and the operations very simple.  Things can get really complicated really quickly.

- How to tackle these tabulations?

# Group-wise Operations

All techniques for this problem rely on the ***split-apply-combine*** strategy

**First,** take the data (or some object) and *split* it into smaller datasets on the basis of some variable

Dataset A

x|y|z
-----|------|-----
1|1|1
2|2|1
3|3|1
4|1|2
5|2|2
6|3|2

Datasets B and C (Dataset A split according to "z") 

x|y|z| | | | | |x|y|z
-----|------|-----|-----|-----|-----|-----|-----|-----|-----|-----
1|1|1| | | | | |4|1|2
2|2|1| | | | | |5|2|2
3|3|1| || | | |6|3|2

**Second,** apply some function to each one of the smaller datasets/objects 

Example function: *mean* of variables "x" and "y"

Datasets B' and C'

mean(x)|mean(y)|z| | | | | |mean(x)|mean(y)|z
-----|------|-----|-----|-----|-----|-----|-----|-----|-----|-----
2|2|1| | | | | |5|2|2

**Third,** combine the results into a larger dataset/object

Datasets B' and C'

mean(x)|mean(y)|z| | | | | |mean(x)|mean(y)|z
-----|------|-----|-----|-----|-----|-----|-----|-----|-----|-----
2|2|1| | | | | |5|2|2

Dataset A'

mean(x)|mean(y)|z
-----|------|-----
2|2|1
5|2|2

# Group-wise Operations/plyr

- *plyr* is the go-to package for all your splitting-applying-combining needs
- Among its many benefits (above base R capabilities):
a) Don't have to worry about different name, argument, or output consistencies
b) Can be parallelized 
c) Input from, and output to, data frames, matricies, and lists
d) Progress bars for lengthy computation
e) Informative error messages

- Note that *dplyr* is the next iteration of *plyr* that focuses on fast data frame manipulations. We won't cover it here, but recommend you take a look at it.


```{r}
# Install the "plyr" package (only necessary one time)
# install.packages("plyr") # Not Run

# Load the "plyr" package (necessary every new R session)
library(plyr)
```

# Group-wise Operations/plyr/selecting functions

- Two essential questions:
1) What is the class of your input object?
2) What is the class of your desired output object?
- If you want to split a **d**ata frame, and return results as a **d**ata frame, you use **dd**ply
- If you want to split a **d**ata frame, and return results as a **l**ist, you use **dl**ply
- If you want to split a **l**ist, and return results as a **d**ata frame, you use **ld**ply

# Group-wise Operations/plyr/writing commands

All of the major plyr functions have the same basic syntax

```{r, eval=FALSE}
xxply(.data=, .variables=, .fun=, ...)
```

Consider the case where we want to calculate tipping behavior across gender, smoking status, and day-of-the-week from a data.frame, then return them as a data.frame:

```{r}
# Calculate tip amount as percent of total bill
tips$tip.pct <- tips$tip/tips$total_bill*100

# Using the appropriate plyr function (ddply), compute average tip percentages
ddply(.data=tips, .variables=.(gender, smoker, day), .fun=summarize, mean.tip.pct=mean(x = tip.pct))
```

Consider the case where we want to calculate tipping behavior across gender, smoking status, and day-of-the-week from a data.frame, then return them as a list:

```{r}
dlply(.data=tips, .variables=.(gender, smoker, day), .fun=summarize, mean.tip.pct=mean(x = tip.pct))
```

Consider the case where we want to calculate vote choice statistics across race from a list, and return them as a data.frame:

```{r}
# Split the data.frame into a list on the basis of gender, smoker status, and day-of-the-week
tips.split <- split(x=tips, f = list(tips$sex, tips$smoker, tips$day))
head(x = tips.split, n = 3)

# Compute summary statistics (note: no .variables argument)
ldply(.data=tips.split, .fun=summarize, mean.tip.pct=mean(x = tip.pct))
```

Consider the case where we want to calculate vote choice statistics across race from a list, and return them as a list:

```{r}
llply(.data=tips.split, .fun=summarize, mean.tip.pct=mean(x = tip.pct))
```

# Group-wise Operations/plyr/functions

- plyr can accomodate any user-defined function, but it also comes with some pre-defined functions that assist with the most common split-apply-combine tasks
- We've already seen **summarize**, which creates user-specified vectors and combines them into a data.frame.  Here are some other helpful functions:

**transform**: applies a function to a data.frame and adds new vectors (columns) to it

```{r}
# Add a column containing the average tip of the gender of the individual on that day
tips.transformed <- ddply(.data=tips, .variables=.(sex, day), .fun=transform, sex.day.avg=mean(x=tip.pct))
head(x = tips.transformed, n = 15)
```

Note that **transform** can't do transformations that involve the results of *other* transformations from the same call

```{r}
# Attempt to add new columns that draw on other (but still new) columns
tips.transformed <- ddply(.data=tips, .variables=.(sex, day), .fun=transform, 
                        sex.day.avg=mean(x=tip.pct),
                        sex.day.avg.deviation=tip.pct-sex.day.avg)
```

For this, we need **mutate**: just like transform, but it executes the commands iteratively so transformations can be carried out that rely on previous transformations from the same call

```{r}
# Attempt to add new columns that draw on other (but still new) columns
tips.transformed <- ddply(.data=tips, .variables=.(sex, day), .fun=mutate, 
                        sex.day.avg=mean(x=tip.pct),
                        sex.day.avg.deviation=tip.pct-sex.day.avg)
head(x = tips.transformed, n = 15)
```

Another very useful function is **arrange**, which orders a data frame on the basis of column contents

```{r}
# Compute average tips across gender, smoker status, and day of the week
tips.summary <- ddply(.data=tips, .variables=.(gender, smoker, day), .fun=summarize, mean.tip.pct=mean(x = tip.pct))

# Arrange summarized data from highest average tip to lowest average tip
arrange(df = tips.summary, mean.tip.pct)

# Arrange summarized data from lowest average tip to highest average tip
arrange(df = tips.summary, desc(mean.tip.pct))
```

# Tidying Data

> It is often said that 80% of data analysis is spent on the process of cleaning and preparing the data. (Dasu and Johnson, 2003)

Even before we conduct analysis or calculations, we need to put our data into the correct format. The goal here is to rearrange a messy dataset into one that is **tidy**

The two most important properties of tidy data are:

1) Each column is a variable.
2) Each row is an observation.

Tidy data is easier to work with, because you have a consistent way of referring to variables (as column names) and observations (as row indices). It then becomes easy to manipulate, visualize, and model.

For more on the concept of *tidy* data, read Hadley Wickham's paper [here](http://vita.had.co.nz/papers/tidy-data.html)

# Tidying Data/Wide vs. Long Formats

> "Tidy datasets are all alike but every messy dataset is messy in its own way." – Hadley Wickham

Tabular datasets can be arranged in many ways. For instance, consider the data below. Each data set displays information on heart rate observed in individuals across 3 different time periods. But the data are organized differently in each table. Which one of these do you think is the *tidy* format?

```{r}
wide <- data.frame(
  name = c("Wilbur", "Petunia", "Gregory"),
  time1 = c(67, 80, 64),
  time2 = c(56, 90, 50),
  time3 = c(70, 67, 101)
)
wide

long <- data.frame(
  name = c("Wilbur", "Petunia", "Gregory", "Wilbur", "Petunia", "Gregory", "Wilbur", "Petunia", "Gregory"),
  time = c(1, 1, 1, 2, 2, 2, 3, 3, 3),
  heartrate = c(67, 80, 64, 56, 90, 50, 70, 67, 10)
)
long
```

In the example above, the first dataframe (the "wide" one) would not be considered *tidy* because values (i.e., heartrate) are spread across multiple columns.

We often refer to these different structurs as "long" vs. "wide" formats. In the "long" format, you usually have 1 column for the observed variable and the other columns are ID variables. 

For the "wide" format each row is often a site/subject/patient and you have multiple observation variables containing the same type of data. These can be either repeated observations over time, or observation of multiple variables (or a mix of both). In the above case, we had the same kind of data (heart rate) entered across 3 different columns, corresponding to three different time periods.

![](img/tidyr-fig1.png)

You may find data input may be simpler or some other applications may prefer the "wide" format. However, many of R’s functions have been designed assuming you have "long" format data. 

# Tidying Data/tidyr

Thankfully, the `tidyr` package will help you efficiently transform your data regardless of original format.

```{r}
# Install the "tidyr" package (only necessary one time)
# install.packages("tidyr") # Not Run

# Load the "reshape2" package (necessary every new R session)
library(tidyr)
```

# Tidying Data/tidyr/gather

Until now, we’ve been using the nicely formatted original gapminder dataset, but 'real' data (i.e. our own research data) will never be so well organized. Here let's start with the wide format version of the gapminder dataset.

```{r}
gap_wide <- read.csv("../data/gapminder_wide.csv", stringsAsFactors = FALSE)
str(gap_wide)
```

The first step towards getting our nice tidy data format is to first convert from the wide to the long format. 

The function `gather()` will 'gather' the observation variables into a single variable. This is sometimes called "melting" your data, because it melts the table from wide to long. 

```{r}
gap_long <- gap_wide %>%
    gather(obstype_year, obs_values, 3:38)
str(gap_long)
```

Notice that we put 3 arguments into the `gather()` function: 

1) the name the new column for the new ID variable (`obstype_year`), 
2) the name for the new amalgamated observation variable (`obs_value`), 
3) the indices of the old observation variables (`3:38`, signalling columns 3 through 38) that we want to gather into one variable. Notice that we don't want to melt down columns 1 and 2, as these are considered "ID" variables.

We can also select observation variables using:

* variable indices
* variable names (without quotes)
* `x:z` to select all variables between x and z
* `-y` to *exclude* y
* `starts_with(x, ignore.case = TRUE)`: all names that starts with `x`
* `ends_with(x, ignore.case = TRUE)`: all names that ends with `x`
* `contains(x, ignore.case = TRUE)`: all names that contain `x`

See the `select()` function in `dplyr` for more options.

For instance, here we do the same thing with (1) the `starts_with` function, and (2) the `-` operator:

```{r}
# with the starts_with() function
gap_long <- gap_wide %>%
    gather(obstype_year, obs_values, starts_with('pop'),
           starts_with('lifeExp'), starts_with('gdpPercap'))
str(gap_long)

# with the - operator
gap_long <- gap_wide %>% 
  gather(obstype_year,obs_values,-continent,-country)
str(gap_long)
```

However you choose to do it, notice that the output collapses all of the measure variables into two columns: one containing new ID variable, the other containing the observation value for that row. 

# Tidying Data/tidyr/separate

You'll notice that in our long dataset, `obstype_year` actually contains 2 pieces of information, the observation type (`pop`, `lifeExp`, or `gdpPercap`) and the year.

We can use the `separate()` function to split the character strings into multiple variables:

```{r}
gap_long_sep <- gap_long %>% 
  separate(obstype_year, into=c('obs_type','year'), sep="_")
gap_long_sep$year <- as.integer(gap_long_sep$year)
```

# Tidying Data/tidyr/spread

The opposite of `gather()` is `spread()`. It spreads our observation variables back out to make a wider table We can use this function to spread our `gap_long()` to the original format.

```{r}
gap_wide <- gap_long_sep %>% 
  spread(obs_type,obs_values)
str(gap_wide)
```

All we need is some quick fixes to make this dataset identical to the original `gapminder` dataset:

```{r}
gapminder <- read.csv("../data/gapminder-FiveYearData.csv")
head(gap_wide)
head(gapminder)

# rearrange columns
gap_wide <- gap_wide[,names(gapminder)]
head(gap_wide)

# arrange by country, continent, and year
gap_wide <- gap_wide %>% 
  arrange(country,continent,year)
head(gap_wide)
```

# Describing Relationships & Causal Inference

- Once we've carried out group-wise operations and perhaps reshaped it, we may also like to attempt describing the relationships in the data or conducting some causal inference

- This often requires doing the following:
1) Estimating Regressions
2) Carryingout Regression Diagnostics

# Inference/Regression

- Running regressions in R is extremely simple, very straightforwd (though doing things with standard errors requires a little extra work)

- Most basic, catch-all regression function in R is *glm*

- *glm* fits a generalized linear model with your choice of family/link function (gaussian, logit, poisson, etc.)

- *lm* is just a standard linear regression (equivalent to glm with family=gaussian(link="identity"))

- The basic glm call looks something like this:

```{r eval=FALSE}
glm(formula=y~x1+x2+x3+..., family=familyname(link="linkname"), data=)
```

- There are a bunch of families and links to use (help(family) for a full list), but some essentials are **binomial(link = "logit")**, **gaussian(link = "identity")**, and **poisson(link = "log")**

- Example: suppose we want to regress the tip percent on the total bill and the size of the party, as well as the gender and smoker status of the tipper.  The glm call would be something like this:

```{r}
# Regress tip percent on total bill and party size
reg <- glm(formula=tip.pct~total_bill+size+sex+smoker, 
                family=gaussian, data=tips)
```

- When we store this regression in an object, we get access to several items of interest

```{r}
# View objects contained in the regression output
objects(reg)
# Examine regression coefficients
reg$coefficients
# Examine regression DoF
reg$df.residual
# Examine regression fit (AIC)
reg$aic
```

- R has a helpful summary method for regression objects
```{r}
summary(reg)
```

- Can also extract useful things from the summary object

```{r}
# Store summary method results
sum.reg <- summary(reg)
# View summary method results objects
objects(sum.reg)
# View table of coefficients
sum.reg$coefficients
```

- Note that, in our results, R has broken up our variables into their different factor levels (as it will do whenever your regressors have factor levels)

- If your data aren't factorized, you can tell glm to factorize a variable (i.e. create dummy variables on the fly) by writing

```{r, eval=FALSE}
glm(formula=y~x1+x2+factor(x3), family=family(link="link"), data=)
```

- There are also some useful shortcuts for regressing on interaction terms:

**x1:x2** interacts all terms in x1 with all terms in x2
```{r}
summary(glm(formula=tip.pct~total_bill+size+sex:smoker, 
                family=gaussian, data=tips))
```

**x1*x2** produces the cross of x1 and x2, or x1+x2+x1:x2
```{r}
summary(glm(formula=tip.pct~total_bill+size+sex*smoker, 
                family=gaussian, data=tips))
```

# Inferences/Regression Diagnostics

- The package *lmtest* has most of what you'll need to run basic regression diagnostics.

- Breusch-Pagan Test for Heteroscedasticity 
```{r}
bptest(reg)
```

- Breusch-Godfrey Test for Higher-order Serial Correlation 
```{r}
bgtest(reg)
```

- Durbin-Watson Test for Autocorrelation of Disturbances
```{r}
dwtest(reg)
```

- Can also estimate heteroskedasticity/autocorrelation consistent standard errors via *coeftest* and the *sandwich* package
```{r}
coeftest(x=reg, vcov.=vcovHC)
```

# Breakout and overnight homework

### Basics

1) Use plyr to create a data frame containing the median departure delay for each destination.

2) Now do the 95th percentile of departure delay for each destination by month pair.

### Using the ideas

3) Use plyr to add a column to the airline dataset that is the total number of flights to the destination of each flight.

4) Use plyr to add a column to the airline dataset that is the hour of the scheduled departure.

5) Use reshape2 to take the result from question #2 and put it in wide format so that destinations are rows and months are columns. 

6) Fit a logistic regression where the outcome is whether there is a departure delay of at least 30 minutes, based on month and day of week and hour of day (see #4). These should all be factor variables when used in the regression. Fit the model for Chicago (ORD) flights. Now fit aseparate model for San Diego (SAN) flights. 

7) Fit separate logistic regressions for a set of 5-10 destinations, including ORD and SAN, all in a single call to dlply.
 
### Advanced

8) How do you predict the probability of a departure delay of more than 30 minutes for a given set of covariate values? Consider the `predict.glm()` function and what its help page says. Or write code that converts from the model coefficients to the probability scale. Compare the predicted probability of a departure delay of more than 30 minutes for a Friday flight in a day in December to that for a Saturday flight in April, both at 2 pm. Finally, see if the predictions vary much depending on whether the predictions are based on city-specific model fits or a single model where the destination is a predictor?





